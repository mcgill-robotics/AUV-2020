{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE PARAMETERS (this is the only thing to modify if you just want to train a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roboflow parameters\n",
    "roboflow_api_key = \"MJQUATZvpcKoBxRjLuXx\"\n",
    "roboflow_workspace_name = \"auv2024\"\n",
    "roboflow_project_name = \"front-cam-felps\"\n",
    "roboflow_project_version = 5\n",
    "\n",
    "# Training parameters\n",
    "target_classes = [\"Buoy\", \"Gate\"]\n",
    "train_test_val_split = (0.7, 0.2, 0.1)\n",
    "model_save_filename = \"best_AUV_sim_front_cam_model.pt\"\n",
    "epoch_increments = 200 # save the model weights to google drive every `epoch_increments` epochs\n",
    "batch_size = -1\n",
    "\n",
    "# Custom augmentation parameters\n",
    "colorAugmentProb = 0.5\n",
    "noiseAugmentProb = 0.5\n",
    "resolutionAugmentProb = 0.5\n",
    "contrastAugmentProb = 0.5\n",
    "blurAugmentProb = 0.5\n",
    "brightnessAugmentProb = 0.0\n",
    "\n",
    "# WARNING: do not change these \"randomly\", check the YoloV8 docs for what these parameters affect before modifying (these values have worked well)\n",
    "# See the last cell for where these parameters are used in the model\n",
    "degrees = 360\n",
    "flipud = 0.5\n",
    "fliplr = 0.5\n",
    "max_perspective_change = 0.001\n",
    "max_translate = 0.1\n",
    "max_scale_change = 0.3\n",
    "mosaic = 0.5\n",
    "mixup = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Python dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "!pip install albumentations\n",
    "!pip install opencv-python\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir data\\augmented\n",
    "!mkdir data\\augmented\\train\n",
    "!mkdir data\\augmented\\test\n",
    "!mkdir data\\augmented\\val\n",
    "!mkdir data\\augmented\\train\\images\n",
    "!mkdir data\\augmented\\test\\images\n",
    "!mkdir data\\augmented\\val\\images\n",
    "!mkdir data\\augmented\\train\\labels\n",
    "!mkdir data\\augmented\\test\\labels\n",
    "!mkdir data\\augmented\\val\\labels\n",
    "!mkdir data\\raw\n",
    "!mkdir data\\raw\\images\n",
    "!mkdir data\\raw\\labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define YOLO classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_absolute_path = os.path.abspath(\"data\")\n",
    "print(\"Data absolute path:\", data_folder_absolute_path)\n",
    "\n",
    "with open('data.yaml', 'w+') as f:\n",
    "    f.write(\"train: {}\\\\augmented\\\\train\\\\images\\n\".format(data_folder_absolute_path))\n",
    "    f.write(\"test: {}\\\\augmented\\\\test\\\\images\\n\".format(data_folder_absolute_path))\n",
    "    f.write(\"val: {}\\\\augmented\\\\val\\\\images\\n\".format(data_folder_absolute_path))\n",
    "    f.write(\"nc: {}\\n\".format(len(target_classes)))\n",
    "    f.write('names: {}'.format(target_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of samples, make two copies of each sample that are darker/brighter to simulate differently lit environments\n",
    "def brightnessAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([ A.augmentations.transforms.ColorJitter (brightness=(1.05, 1.05), contrast=0, saturation=0, hue=0, always_apply=True) ])\n",
    "        bright_img = transform(image=image)[\"image\"]\n",
    "        transform = A.Compose([ A.augmentations.transforms.ColorJitter (brightness=(0.95, 0.95), contrast=0, saturation=0, hue=0, always_apply=True) ])\n",
    "        dark_img = transform(image=image)[\"image\"]\n",
    "        out.append(bright_img)\n",
    "        out.append(dark_img)\n",
    "    return out\n",
    "\n",
    "#given a list of samples, make a copy of each sample but more blurred to simulate objects out of focus, dirty lenses, and backscattering\n",
    "def blurAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        ksize = (10, 10) # lower to lower blur\n",
    "        blurred_img = cv2.blur(image, ksize)\n",
    "        out.append(blurred_img)\n",
    "    return out\n",
    "\n",
    "#given a list of samples, make a copy of each sample but with a lower contrast image to simulate backscattering and over/under-exposure\n",
    "def contrastAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([ A.augmentations.transforms.ColorJitter (brightness=0, contrast=(0.1, 0.1), saturation=0, hue=0, always_apply=True) ])\n",
    "        decontrasted_img = transform(image=image)[\"image\"]\n",
    "        out.append(decontrasted_img)\n",
    "    return out\n",
    "\n",
    "#given a list of samples, make a copy of each sample but with camera noise added to the image to simulate different camera feeds\n",
    "def noiseAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        transform = A.Compose([ A.augmentations.transforms.ISONoise(color_shift=(0.01, 0.01), intensity=(0.8, 0.8), always_apply=True) ])\n",
    "        noisy_img = transform(image=image)[\"image\"]\n",
    "        out.append(noisy_img)\n",
    "    return out\n",
    "\n",
    "#given a list of samples, make a copy of each sample but with the image downscaled (lower resolution of image) to simulate lower quality cameras/images\n",
    "def resolutionAugment(images):\n",
    "    out = []\n",
    "    for image in images:\n",
    "        #interpolation=A.augmentations.transforms.Interpolation(downscale=cv2.INTER_NEAREST, upscale=cv2.INTER_NEAREST)\n",
    "        transform = A.Compose([ A.augmentations.transforms.Downscale(scale_min=0.25, scale_max=0.25, always_apply=True) ])\n",
    "        low_res_img = transform(image=image)[\"image\"]\n",
    "        out.append(low_res_img)\n",
    "    return out\n",
    "\n",
    "#increase intensity of blues in given image\n",
    "def make_bluer(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) #split by channel\n",
    "    img_b = np.uint16(img_b)\n",
    "    img_b += color_shift_intensity\n",
    "    np.clip(img_b, 0, 255, out=img_b)\n",
    "    img_b = np.uint8(img_b)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) #merge adjusted channels\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "#increase intensity of greens in given image\n",
    "def make_greener(img, color_shift_intensity):\n",
    "    img_b, img_g, img_r = cv2.split(img) #split by channel\n",
    "    img_g = np.uint16(img_g)\n",
    "    img_g += color_shift_intensity\n",
    "    np.clip(img_g, 0, 255, out=img_g)\n",
    "    img_g = np.uint8(img_g)\n",
    "    img = cv2.merge((img_b, img_g, img_r)) #merge adjusted channels\n",
    "    del img_b\n",
    "    del img_g\n",
    "    del img_r\n",
    "    return img\n",
    "\n",
    "#given a list of samples, make two copies of each sample (one bluer, one greener) to simulate different pools + color attenuation\n",
    "def colorAugment(images):\n",
    "    out = []\n",
    "    color_shift_intensity = int(255*0.1)\n",
    "    for image in images:\n",
    "        blue_img = make_bluer(image, color_shift_intensity)\n",
    "        green_img = make_greener(image, color_shift_intensity)\n",
    "        out.append(blue_img)\n",
    "        out.append(green_img)\n",
    "    return out\n",
    "\n",
    "#given a single image and augmentation function, displays the image before and images after augmentation\n",
    "def visualizeAugmentation(img, aug):\n",
    "    #show original image\n",
    "    cv2.imshow('og', img)\n",
    "    cv2.waitKey(0)\n",
    "    #show all augmented images\n",
    "    for augmented in aug([(img, \"\")])[1:]:\n",
    "        cv2.imshow('augmented',augmented[0])\n",
    "        cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(source_folder):\n",
    "    label_filenames = []\n",
    "    img_filenames = [f for f in listdir(source_folder + '\\\\images') if isfile(join(source_folder + '\\\\images', f))]\n",
    "    for img_filename in img_filenames:\n",
    "        label_filenames.append(os.path.splitext(img_filename)[0] + \".txt\")\n",
    "\n",
    "    return np.array(img_filenames), np.array(label_filenames)\n",
    "\n",
    "def split_file_names(images,labels,splits):\n",
    "    perm = np.random.permutation(len(images))\n",
    "    images = images[perm]\n",
    "    labels = labels[perm]\n",
    "    splits = [int(len(images)*s) for s in splits]\n",
    "    train_images = images[:splits[0]]\n",
    "    train_labels = labels[:splits[0]]\n",
    "    val_images = images[splits[0]: splits[0] + splits[1]]\n",
    "    val_labels = labels[splits[0]: splits[0] + splits[1]]\n",
    "    test_images = images[splits[0] + splits[1]:]\n",
    "    test_labels = labels[splits[0] + splits[1]:]\n",
    "    return train_images, train_labels, val_images, val_labels, test_images, test_labels\n",
    "\n",
    "def get_augs(img_filename,source_folder):\n",
    "    img = cv2.imread(source_folder + '\\\\images\\\\' + img_filename)\n",
    "    augs = [img]\n",
    "    if(np.random.rand() < colorAugmentProb):\n",
    "        augs = augs + colorAugment(augs)\n",
    "    if(np.random.rand() < noiseAugmentProb):\n",
    "        augs = augs + noiseAugment(augs)\n",
    "    if(np.random.rand() < resolutionAugmentProb):\n",
    "        augs = augs + resolutionAugment(augs)\n",
    "    if(np.random.rand() < contrastAugmentProb):\n",
    "        augs = augs + contrastAugment(augs)\n",
    "    if(np.random.rand() < blurAugmentProb):\n",
    "        augs = augs + blurAugment(augs)\n",
    "    if(np.random.rand() < brightnessAugmentProb):\n",
    "        augs = augs + brightnessAugment(augs)\n",
    "    return augs\n",
    "\n",
    "def do_augs_and_export(img_filenames,label_filenames,source_folder,output_folder):\n",
    "    name_num = 1\n",
    "    for (img_filename,label_filename) in zip(img_filenames,label_filenames):\n",
    "        augs = get_augs(img_filename,source_folder)\n",
    "        with open(source_folder + \"\\\\labels\\\\\" + label_filename) as f:\n",
    "            #build array of bounding boxes (each line its own element)\n",
    "            bounding_boxes = f.read()\n",
    "        for aug in augs:\n",
    "            cv2.imwrite(output_folder + '\\\\images\\\\img' + str(name_num) + '.png', aug)\n",
    "            with open(output_folder + '\\\\labels\\\\img' + str(name_num) + '.txt',\"w+\") as f:\n",
    "                f.write(bounding_boxes)\n",
    "            name_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=roboflow_api_key)\n",
    "project = rf.workspace(roboflow_workspace_name).project(roboflow_project_name)\n",
    "version = project.version(roboflow_project_version)\n",
    "dataset = version.download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"{}-{}\".format(roboflow_project_name, roboflow_project_version)\n",
    "roboflow_folder_absolute_path = os.path.abspath(folder_name)\n",
    "data_folder_absolute_path = os.path.abspath(\"data\")\n",
    "\n",
    "print(\"Roboflow folder name:\", folder_name)\n",
    "print(\"Roboflow absolute path:\", roboflow_folder_absolute_path)\n",
    "print(\"Data absolute path:\", data_folder_absolute_path)\n",
    "\n",
    "!move \"{roboflow_folder_absolute_path}\\train\\images\\*.*\" \"{data_folder_absolute_path}\\raw\\images\\\"\n",
    "!move \"{roboflow_folder_absolute_path}\\train\\labels\\*.*\" \"{data_folder_absolute_path}\\raw\\labels\\\"\n",
    "!move \"{roboflow_folder_absolute_path}\\valid\\images\\*.*\" \"{data_folder_absolute_path}\\raw\\images\\\"\n",
    "!move \"{roboflow_folder_absolute_path}\\valid\\labels\\*.*\" \"{data_folder_absolute_path}\\raw\\labels\\\"\n",
    "!move \"{roboflow_folder_absolute_path}\\test\\images\\*.*\" \"{data_folder_absolute_path}\\raw\\images\\\"\n",
    "!move \"{roboflow_folder_absolute_path}\\test\\labels\\*.*\" \"{data_folder_absolute_path}\\raw\\labels\\\"\n",
    "!rmdir /s /q \"{folder_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data, split into train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"data\\\\augmented\"\n",
    "in_folder = \"data\\\\raw\"\n",
    "\n",
    "img_names, label_names = get_file_names(in_folder)\n",
    "num_raw_samples = len(img_names)\n",
    "train_images, train_labels, val_images, val_labels, test_images, test_labels = split_file_names(img_names,label_names,train_test_val_split)\n",
    "do_augs_and_export(train_images,train_labels,in_folder,out_folder + \"\\\\train\")\n",
    "do_augs_and_export(val_images,val_labels,in_folder,out_folder + \"\\\\val\")\n",
    "do_augs_and_export(test_images,test_labels,in_folder,out_folder + \"\\\\test\")\n",
    "\n",
    "augmented_train_img_names, _ = get_file_names(out_folder + \"\\\\train\")\n",
    "augmented_test_img_names, _ = get_file_names(out_folder + \"\\\\test\")\n",
    "augmented_val_img_names, _ = get_file_names(out_folder + \"\\\\val\")\n",
    "\n",
    "num_augmented_samples = len(augmented_train_img_names) + len(augmented_test_img_names) + len(augmented_val_img_names)\n",
    "\n",
    "print(\"Augmentation completed. Went from {} raw samples to a total of {} after augmentation.\".format(num_raw_samples, num_augmented_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CUDA dependencies and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available!!!\n"
     ]
    }
   ],
   "source": [
    "CUDA_setup_is_valid = int(torch.cuda.is_available() and torch.cuda.device_count())\n",
    "\n",
    "print(\"Cuda is {}available!!!\".format(\"\" if CUDA_setup_is_valid else \"NOT \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rmdir /s /q \"runs\"\n",
    "!mkdir \"runs\"\n",
    "!mkdir \"runs/detect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.22 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.21  Python-3.12.3 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=c:\\Users\\poten\\AUV-2024\\catkin_ws\\src\\vision\\model_pipeline\\data.yaml, epochs=200, time=None, patience=100, batch=1, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=2, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=360, translate=0.1, scale=0.3, shear=0.0, perspective=0.001, flipud=0.5, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.5, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011238 parameters, 3011222 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\poten\\AUV-2024\\catkin_ws\\src\\vision\\model_pipeline\\data\\augmented\\train\\labels.cache... 695 images, 145 backgrounds, 0 corrupt: 100%|██████████| 695/695 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Got processor for bboxes, but no transform to process it.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\poten\\AUV-2024\\catkin_ws\\src\\vision\\model_pipeline\\data\\augmented\\val\\labels.cache... 191 images, 27 backgrounds, 0 corrupt: 100%|██████████| 191/191 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/200     0.585G      1.852      3.774      1.625          3        640: 100%|██████████| 695/695 [00:40<00:00, 17.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:03<00:00, 27.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227      0.424      0.478      0.488      0.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/200     0.587G      1.717      3.263      1.591          3        640: 100%|██████████| 695/695 [00:37<00:00, 18.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 33.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227       0.41      0.348      0.374       0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/200     0.608G      1.707      2.939      1.621          2        640: 100%|██████████| 695/695 [00:36<00:00, 18.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 33.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227      0.546      0.538       0.58      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/200     0.608G      1.699      2.528      1.584          1        640: 100%|██████████| 695/695 [00:36<00:00, 18.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 33.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227      0.487      0.537      0.487      0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/200     0.608G        1.7      2.432      1.574          3        640: 100%|██████████| 695/695 [00:39<00:00, 17.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 33.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227      0.525      0.281      0.412      0.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/200     0.608G      1.668      2.177       1.58          3        640: 100%|██████████| 695/695 [00:37<00:00, 18.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 33.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227       0.36      0.452      0.374      0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/200      0.61G      1.677      2.121       1.57          1        640: 100%|██████████| 695/695 [00:36<00:00, 18.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:02<00:00, 32.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        191        227      0.393      0.448      0.415      0.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/200     0.608G      1.716        2.1      1.657          2        640:  11%|█         | 78/695 [00:04<00:34, 17.93it/s]"
     ]
    }
   ],
   "source": [
    "data_yaml_file_absolute_path = os.path.abspath(\"data.yaml\")\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\") #load a pretrained model\n",
    "\n",
    "# Start the training process\n",
    "while True:\n",
    "    try:\n",
    "        model.train(\n",
    "            data=data_yaml_file_absolute_path,\n",
    "            epochs=epoch_increments,\n",
    "            # device=0, # --> it can automatically detect the device available\n",
    "            batch=1,\n",
    "            degrees=degrees,\n",
    "            flipud=flipud,\n",
    "            fliplr=fliplr,\n",
    "            perspective=max_perspective_change,\n",
    "            translate=max_translate,\n",
    "            scale=max_scale_change,\n",
    "            mosaic=mosaic,\n",
    "            mixup=mixup,\n",
    "            pretrained=True,\n",
    "            task='detect',\n",
    "            cache=False,\n",
    "            workers=2,\n",
    "        )\n",
    "        shutil.copyfile(\"runs\\\\detect\\\\train\\\\weights\\\\best.pt\", model_save_filename)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Caught a RuntimeError: {e}\")\n",
    "        break  # Break out of the loop if an error occurs to prevent infinite loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
